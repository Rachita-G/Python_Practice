{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachita-G/Python_Practice/blob/main/Model_Concepts/Train_Test_Split_and_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTwJOdbXTMhh"
      },
      "source": [
        "# TRAIN TEST SPLIT\n",
        "\n",
        "Evaluating a Machine Learning model can be quite tricky. Usually, we split the data set into training and testing sets and use the training set to train the model and testing set to test the model. We then evaluate the model performance based on an error metric to determine the accuracy of the model. \n",
        "\n",
        "In scikit-learn a random split into training and test sets can be quickly computed with the train_test_split helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRfe5MWXTMhl"
      },
      "outputs": [],
      "source": [
        "# In this lesson we will explore the train_test_split module\n",
        "# Therefore we need no more than the module itself and NumPy\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7fcbd-9TMhn"
      },
      "outputs": [],
      "source": [
        "# Let's generate a new data frame 'a' which will contain all integers from 1 to 100\n",
        "# The method np.arange works like the built-in method 'range' with the difference it creates an array\n",
        "a=np.arange(1,101) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhjplsQNTMhn",
        "outputId": "67731314-3d47-4578-d29b-f84ecdab0cda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
              "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
              "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
              "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
              "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
              "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
              "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
              "        92,  93,  94,  95,  96,  97,  98,  99, 100])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky9xXgXhTMho",
        "outputId": "a3e02d38-4938-4735-d1b4-af37189cd982"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513,\n",
              "       514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526,\n",
              "       527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539,\n",
              "       540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552,\n",
              "       553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565,\n",
              "       566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578,\n",
              "       579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591,\n",
              "       592, 593, 594, 595, 596, 597, 598, 599, 600])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Similarly, let's create another ndarray 'b', which will contain integers from 501 to 600\n",
        "# We have intentionally picked these numbers so we can easily compare the two\n",
        "# Obviously, the difference between the elements of the two arrays is 500 for any two corresponding elements\n",
        "b = np.arange(501,601)\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ntqcqa4tTMho",
        "outputId": "9ed30b98-3419-4699-fe23-698007c5b59a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([ 14,  63,  28,  65,  10,  47,  33,  41,  38,  26,  97,  39,  32,\n",
              "         25,  21,  18,  87,  86,  40,  78,  55,  70,  45, 100,  81,  84,\n",
              "         52,   4,  99,  98,  88,  53,  46,  29,  27,  66,   3,  60,  72,\n",
              "         15,  69,  37,  24,  48,  96,   5,  56,  30,  22,   1,  36,  71,\n",
              "         76,  67,  80,  59,   2,  42,  95,  74,  19,  94,  89,  85,  11,\n",
              "         73,  16,  75,   7,  23,  31,   8,  79,  17,  12]),\n",
              " array([57, 54, 50, 43, 82, 35, 83, 13, 61,  9, 93, 92, 44,  6, 90, 64, 34,\n",
              "        58, 49, 68, 77, 62, 20, 91, 51])]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_test_split(a) # splitting a in 2 arrays randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGPwnY-gTMhp"
      },
      "outputs": [],
      "source": [
        "# There are several different arguments we can set when we employ this method\n",
        "# Most often, we have inputs and targets, so we have to split 2 different arrays\n",
        "# we are simulating this situation by splitting 'a' and 'b'\n",
        "\n",
        "# You can specify the 'test_size' or the 'train_size' (but the latter is deprecated and will be removed)\n",
        "# essentially the two have the same meaning \n",
        "# Common splits are 75-25, 80-20, 85-15, 90-10\n",
        "\n",
        "# Finally, you should always employ a 'random_state'\n",
        "# In this way you ensure that when you are splitting the data you will always get the SAME random shuffle\n",
        "\n",
        "# Note 2 arrays (a and b) will be split into 4\n",
        "# The order is train1, test1, train2, test2 \n",
        "# It is very useful to store them in 4 variables, so we can later use them\n",
        "a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=365)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtNax4mOTMhp",
        "outputId": "19a45e3c-801d-404b-d1c6-7002cc04c4eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 25,  32,  99,  73,  91,  66,   3,  59,  94,   1,   8,  15,  90,\n",
              "        54,  31,  20,  77,  82,  30,  35,  95,  42,  38,   7,  11,  50,\n",
              "        21,  48,   2,  17,  10,  58,  68,  43,  41,  16,  88,  72,  79,\n",
              "       100,  80,  39,  24,  86,  22,  23,  62,  76,  18,  47,  55,  26,\n",
              "        60,  19,  71,  64,  51,  63,  65,  28,  12,  78,  13,  44,  75,\n",
              "        87,  40,   4,  29,  49,  37,  57,  27,  74,   6,  45,  92,  34,\n",
              "        53,  83])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLvcY8gpTMhq",
        "outputId": "f80bf473-87ba-4139-a1b3-3c3f3454d8e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 9, 69, 81, 56, 33, 93, 84, 61, 46, 89, 85, 67, 97,  5, 70, 36, 98,\n",
              "       96, 14, 52])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT8wt5SgTMhq",
        "outputId": "722016f4-cabd-4d40-f5e6-ef77463fb07b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([525, 532, 599, 573, 591, 566, 503, 559, 594, 501, 508, 515, 590,\n",
              "       554, 531, 520, 577, 582, 530, 535, 595, 542, 538, 507, 511, 550,\n",
              "       521, 548, 502, 517, 510, 558, 568, 543, 541, 516, 588, 572, 579,\n",
              "       600, 580, 539, 524, 586, 522, 523, 562, 576, 518, 547, 555, 526,\n",
              "       560, 519, 571, 564, 551, 563, 565, 528, 512, 578, 513, 544, 575,\n",
              "       587, 540, 504, 529, 549, 537, 557, 527, 574, 506, 545, 592, 534,\n",
              "       553, 583])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b_train # notice-- does the same 25 ka 525 n ol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARUu4ve1TMhr",
        "outputId": "0f5af96a-d787-47d3-e53c-b2d04835bcf4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([509, 569, 581, 556, 533, 593, 584, 561, 546, 589, 585, 567, 597,\n",
              "       505, 570, 536, 598, 596, 514, 552])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9SNZJlLTMhr"
      },
      "outputs": [],
      "source": [
        "# to split it order wise only . no shuffling then.\n",
        "a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUqwoodTTMhr",
        "outputId": "475e3806-c0a8-4a2a-b370-746c3294433b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyOraCdnTMhs",
        "outputId": "ae99ab8d-e41b-4ed1-f1ba-eb69bfef8e70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
              "        94,  95,  96,  97,  98,  99, 100])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUL6-IhWTMhs",
        "outputId": "49af9b05-d34f-4a97-e656-9c2379d67ce5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((20,), (80,))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b_test.shape,b_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXeZDuRBTMhs",
        "outputId": "b9087cc5-9586-4580-b0f6-6766b1ca8a56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593,\n",
              "       594, 595, 596, 597, 598, 599, 600])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCsN-a3dTMhs",
        "outputId": "f52b5fe9-b767-4c3f-b9b6-a2318663c0ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513,\n",
              "       514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526,\n",
              "       527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539,\n",
              "       540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552,\n",
              "       553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565,\n",
              "       566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578,\n",
              "       579, 580])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6y4KRiMTMht",
        "outputId": "e87cd591-e153-48bc-8871-dc852703e4a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((150, 4), (150,))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# using iris data\n",
        "from sklearn import datasets\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MzQ9DLXTMht",
        "outputId": "3479470a-08d5-4502-e451-4aaaba4758a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((90, 4), (90,), (60, 4), (60,))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test,y_train, y_test = train_test_split(X,y, test_size=0.4)\n",
        "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t15lD2WHTMht",
        "outputId": "22e0abe4-3dbc-4df7-bcf7-92e6568e287a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "clf=svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_XEnRfHTMht"
      },
      "source": [
        "# Problem in Train-Test Split\n",
        "\n",
        "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
        "\n",
        "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
        "\n",
        "A solution to this problem is a procedure called cross-validation (CV for short). \n",
        "\n",
        "# Cross Validation\n",
        "\n",
        "Under CV, A test set is to be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called \n",
        "K-fold Cross Validation(CV)  training data split into k folds and ensuring that each fold is used as a testing set at some point. \n",
        "K-Fold CV is where a given data set is split into a K number of sections/folds where each fold is used as a testing set at some point. Lets take the scenario of 5-Fold cross validation(K=5). \n",
        "\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The general procedure is as follows:\n",
        "\n",
        "* Shuffle the dataset randomly.\n",
        "* Split the dataset into k groups\n",
        "* For each unique group:\n",
        "       a. Take one group as a hold out or test data set\n",
        "       b. Take the remaining groups as a training data set (consisting of k-1 folds)\n",
        "* Fit a model on the training set and evaluate it on the test set\n",
        "* Retain the evaluation score and discard the model\n",
        "* Summarize the skill of the model using the sample of model evaluation scores\n",
        " This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
        "\n",
        "\n",
        "### Note\n",
        "Cross-Validation is primarily used in scenarios where prediction is the main aim, and the user wants to estimate how well and accurately a predictive model will perform in real-world situations.\n",
        "\n",
        "Cross-Validation seeks to define a dataset by testing the model in the training phase to help minimize problems like overfitting and underfitting. However, you must remember that both the validation and the training set must be extracted from the same distribution, or else it would lead to problems in the validation phase.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWu9VTGITMhu"
      },
      "source": [
        "\n",
        "\n",
        "### Benefits of Cross-Validation\n",
        "* It helps evaluate the quality of your model.\n",
        "* It helps to reduce/avoid problems of overfitting and underfitting.\n",
        "* It lets you select the model that will deliver the best performance on unseen data.\n",
        "\n",
        "### Trade-offs Between Cross-Validation and Train-Test Split\n",
        "\n",
        "* Cross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, it can take more time to run, because it estimates models once for each fold. So it is doing more total work.\n",
        "\n",
        "* Given these tradeoffs, when should you use each approach? On small datasets, the extra computational burden of running cross-validation isn't a big deal. These are also the problems where model quality scores would be least reliable with train-test split. So, if your dataset is smaller, you should run cross-validation.For the same reasons, a simple train-test split is sufficient for larger datasets. It will run faster, and you may have enough data that there's little need to re-use some of it for holdout.\n",
        "\n",
        "* There's no simple threshold for what constitutes a large vs small dataset. If your model takes a couple minute or less to run, it's probably worth switching to cross-validation. If your model takes much longer to run, cross-validation may slow down your workflow more than it's worth.\n",
        "\n",
        "* Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment gives the same results, train-test split is probably sufficient.\n",
        "\n",
        "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\n",
        "\n",
        "The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 10 consecutive times (with different splits each time):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPKzo1wnTMhu",
        "outputId": "242beb2b-3750-493e-ef64-dfe289a41915"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.        , 0.93333333, 1.        , 1.        , 0.86666667,\n",
              "       1.        , 0.93333333, 1.        , 1.        , 1.        ])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "clf=svm.SVC(kernel='linear', C=1)\n",
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV4b5n9yTMhv"
      },
      "source": [
        "The mean score and the 95% confidence interval of the score estimate are hence given by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i20TdYksTMhv",
        "outputId": "f7130f04-2cf6-4dae-ebba-6ce4340e71ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.97 (+/- 0.09)\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOhI0oeoTMhv",
        "outputId": "ecab0d04-0cf2-4900-f3c8-8ee23678f2f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.        , 0.93265993, 1.        , 1.        , 0.86111111,\n",
              "       1.        , 0.93265993, 1.        , 1.        , 1.        ])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "scores=cross_val_score(clf,X,y,cv=10,scoring='f1_macro')\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buAmNZbgTMhv"
      },
      "source": [
        "### Configuration of k\n",
        "The k value must be chosen carefully for your data sample.\n",
        "\n",
        "A poorly chosen value for k may result in a mis-representative idea of the skill of the model, such as a score with a high variance (that may change a lot based on the data used to fit the model), or a high bias, (such as an overestimate of the skill of the model).\n",
        "\n",
        "### Variations on Cross-Validation\n",
        "There are a number of variations on the k-fold cross validation procedure.\n",
        "\n",
        "Three commonly used variations are as follows:\n",
        "\n",
        "* Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
        "* Repeated: This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample.\n",
        "* Stratified: The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n",
        "* LOOCV: Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqtyaLqBTMhv"
      },
      "source": [
        "### 1. KFold\n",
        "KFold divides all the samples in  groups of samples, called folds, of equal sizes (if possible). The prediction function is learned using  folds, and the fold left out is used for test.\n",
        "\n",
        "It takes as arguments the number of splits, whether or not to shuffle the sample, and the seed for the pseudorandom number generator used prior to the shuffle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3JweUzfTMhv",
        "outputId": "40172066-77ca-47a9-daba-2f23fcb3e634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN: [2 3] TEST: [0 1]\n",
            "TRAIN: [0 1 3] TEST: [2]\n",
            "TRAIN: [0 1 2] TEST: [3]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "X = np.array([\"l\",\"m\",\"o\",\"p\"])\n",
        "rkf = KFold(n_splits=3)\n",
        "for train_index, test_index in rkf.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2q22XiATMhw"
      },
      "source": [
        "### 2. RepeatedKFold \n",
        "RepeatedKFold repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition.\n",
        "Repeats K-Fold n times with different randomization in each repetition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHb2wWDlTMhw",
        "outputId": "bde1a1d1-4417-4e26-ad92-6508622c9c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN: [0 4] TEST: [1 2 3]\n",
            "TRAIN: [1 2 3] TEST: [0 4]\n",
            "TRAIN: [0 1] TEST: [2 3 4]\n",
            "TRAIN: [2 3 4] TEST: [0 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "X = np.array([[18,77], [66,87], [51, 62], [83, 84],[9,8]])\n",
        "rkf = RepeatedKFold(n_splits=2, n_repeats=2)\n",
        "for train_index, test_index in rkf.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2UBs43ETMhw"
      },
      "source": [
        "### 3. Repeated Stratified KFold \n",
        "Repeats Stratified K-Fold n times with different randomization in each repetition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWpgR5FOTMhw",
        "outputId": "a24d5956-39ca-49da-875b-5165bc050eaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN: [0 1 3] TEST: [2 4]\n",
            "TRAIN: [2 3 4] TEST: [0 1]\n",
            "TRAIN: [0 1 2 4] TEST: [3]\n",
            "TRAIN: [1 3 4] TEST: [0 2]\n",
            "TRAIN: [0 2 3] TEST: [1 4]\n",
            "TRAIN: [0 1 2 4] TEST: [3]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "X = np.array([[18,77], [66,87], [51, 62], [83, 84],[9,8]])\n",
        "rkf = RepeatedKFold(n_splits=3, n_repeats=2)\n",
        "for train_index, test_index in rkf.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-bxkRKyTMhw"
      },
      "source": [
        "### 4. LeaveOneOut (or LOO) \n",
        "LOO is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for  samples, we have n different training sets and n different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bApOC6azTMhw",
        "outputId": "8f43d041-dbc1-47ef-b2c7-f6d95b391733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 2 3] [0]\n",
            "[0 2 3] [1]\n",
            "[0 1 3] [2]\n",
            "[0 1 2] [3]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "X=[1,2,3,4]\n",
        "loo=LeaveOneOut()\n",
        "for train_index,test_index in loo.split(X):\n",
        "    print(train_index,test_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3St9Y5KTMhw"
      },
      "source": [
        "LOO is more computationally expensive than k-fold cross validation.\n",
        "\n",
        "In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since  of the  samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.\n",
        "\n",
        "However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.\n",
        "\n",
        "### Conclusion\n",
        "As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}